\chapter{Resultados potenciais}

No capítulo passado, vimos que
$f(y|do(x))$ nos permite entender
o comportamento de $Y$ em 
um cenário distinto dos dados observados.
Por exemplo, se $X$ é 
a indicadora de um tratamento e
$Y$ é a indicadora de cura, então
$f(y|do(X=1))$ nos permite entender
a proporção de cura em um cenário hipotético em que
administramos o tratamento a todos os indivíduos.
Esta distribuição nos permite investigar
questões causais que 
não eram acessíveis usando apenas
a distribuição observacional, $f(y,x)$.

Contudo, algumas perguntas causais 
não são respondidas utilizando apenas
os mecanismos desenvolvidos no \cref{cap:intervencao}.
Por exemplo, qual a probabilidade de que 
um indivíduo se cure quando recebe o tratamento e
não se cure quando não o recebe.
Quando tentamos traduzir esta questão,
notamos que partes dela envolvem $Y=1$ e $do(X=1)$
e outras partes envolvem $Y=0$ e $do(X=0)$.
Se tentarmos uma tradução ingênua,
podemos obter uma expressão como $\P(Y=1, Y=0|do(X=1),do(X=0))$.
Contudo, a probabilidade acima não responde à pergunta colocada.
Em primeiro lugar, não está definido fazermos
as intervenções $do(X=1)$ e $do(X=0)$ na mesma unidade amostral.
Além disso, mesmo que a probabilidade estivesse definida,
é impossível que o mesmo $Y$ assuma tanto o valor $1$ quanto $0$.
Isto é, $\P(Y=1, Y=0|\ldots) = 0$.

A última constatação nos revela que
o modelo no \cref{cap:intervencao}
não tem variáveis suficientes para
traduzir a pergunta levantada.
Se imaginamos que é possível que
um indivíduo se cure ao receber o tratamento e
não se cure quando não o recebe, 
isto ocorre pois as ocorrências de cura
em cada cenário hipotético não são 
logicamente equivalentes.
Em outras palavras, é como se 
houvessem \textit{resultados potenciais}\footnote{
 Esta é uma tradução livre da expressão
 ``potential outcomes'' usada em inglês.},
$Y_1$ e $Y_0$, para
indicar a ocorrência de cura em
cada cenário considerado.
Com o uso destas variáveis, 
poderíamos escrever 
$\P(Y_1 = 1, Y_0 = 0)$.

O objetivo desta seção é 
incluir este tipo de variável de forma
a preservar as ferramentas desenvolvidas
no \cref{cap:intervencao}.\footnote{
 Para tal, adotaremos uma construção
 baseada em \citet{Galles1998}.
}
Neste quesito, a maior dificuldade será
estabelecer a distribuição conjunta
entre os resultados potenciais.
Para tal, será útil relembrar
um lema fundamental em simulação:

\begin{lemma}
 \label{lemma:simulacao}
 Considere que $F(v|Pa(V))$ é
 uma função de densidade acumulada condicional arbitrária e
 $U \sim U(0,1)$. Se definirmos,
 $V \equiv F^{-1}(U|Pa(V))$, então
 $V|Pa(V) \sim F$.
\end{lemma}

O \cref{lemma:simulacao} traz 
várias interpretações que nos serão úteis.
A primeira interpretação, de caráter técnico, é 
que podemos simular de 
qualquer distribuição multivariada utilizando apenas
variáveis i.i.d. e funções determinísticas.
Em particular, podemos 
reescrever um SCM de tal forma que
cada vértice, $V$, seja função determinística
de seus pais e uma variável de ruído, $U_V$.
Esta abordagem, que está ligada a
modelos de equações estruturais, é
apresentada nas \cref{def:grafo_struct,def:scm}.

\begin{definition}
 \label{def:grafo_struct}
 Seja $\sG = (\sV, \sE)$ um grafo causal.
 O grafo causal estrutural,
 $\sG^+ = (\sV^+, \sE^+)$, é tal que 
 $\sV^+ = \sV \cup (U_V)_{V \in \sV}$ e
 $\sE^+ = \sE \cup \{(U_V, V): V \in \sV\}$.
 Isto é, para cada $V \in \sV$, 
 $\sG^+$ adiciona uma nova variável $U_V$ e
 uma aresta de $U_V$ a $V$.
\end{definition} 
 
\begin{definition}
 \label{def:scm}
 Seja $(\sG,f)$ um CM.
 O Modelo Estrutural Causal (SCM) para $(\sG,f)$, 
 $(\sG^+,f^+)$, é tal que
 $\sG^+$ é o grafo causal estrutural de $\sG$,
 $(U_V)_{V \in \sV}$ são independentes segundo $f^+$ e,
 para cada $V \in \sV$, existe uma função determinística, 
 $g_V: U_V \times Pa(V) \rightarrow \Re$,
 tal que $f^+(V|U_V,Pa(V)) = \I(V = g_V(U_V, Pa(V)))$ e
 $f^+(\sV) = f(\sV)$.
\end{definition}
 
O \cref{ex:eq_struct} ilustra
uma forma de obter 
um SCM em equações estruturais
a partir de um SCM com dois vértices.
 
\begin{example}
 \label{ex:eq_struct}
 Considere que $X \rightarrow Y$,
 $X \sim \text{Exp}(1)$ e $Y|X \sim \text{Exp}(X)$.
 Neste caso, o grafo estrutural causal é
 dado por $U_X \rightarrow X \rightarrow Y \leftarrow U_Y$.
 Além disso, existem várias representações do SCM
 em equações estruturais. Uma possibilidade é
 definir que $U_X$ e $U_Y$ são i.i.d. e $U(0,1)$,
 $X \equiv -\log(U_X)$ e $Y \equiv -\log(U_Y)/X$.
\end{example}
 
O \cref{lemma:simulacao} também permite
uma interpretação de caráter mais filosófico.
Podemos imaginar que toda variável em um SCM, $V$, é
uma função determinística de seus pais e
de \textit{condições locais} não-observadas, $U_V$.
A expressão ``condições locais'' indica que
cada $U_V$ é usada somente para gerar $V$ e que
as variáveis em $U$ são independentes, isto é,
não trazem informação umas sobre as outras.

A interpretação acima é usada
na definição de resultados potenciais.
A ideia principal é que 
as mesmas funções determínistas e
variáveis de ruído locais são usadas
para gerar todos os resultados potenciais.
A única diferença é que,
para cada resultado potencial,
o valor das variáveis em que 
houve intervenção é fixado.
Esta definição é compatível com 
a ideia de que não é possível modificar
os ruídos locais por meio da intervenção.
Em outras palavras, 
o resultado potencial é o mais próximo possível
do resultado observado sob a restrição que
fixamos os valores das variáveis em que
houve intervenção.

\begin{definition}
 \label{def:grafo_potencial}
 Seja $(\sG, f)$ um CM de grafo causal 
 $\sG = (\sV, \sE)$ e
 $(\sG^+, f^+)$ o seu SCM.
 O grafo de resultados potenciais dado
 por intervenções em $\X \subseteq \sV$,
 $\sG^* = (\sV^*, \sE^*)$ é tal que
 \begin{align*}
  \sV^* =& \{W_{\V=\bv}: W \in \sV, \V \subseteq \X, \bv \in supp(\V)\} \cup \{U_W: W \in \sV\}, \\
  \sE^* =& \{(W_{\V=\bv}, Z_{\V=\bv}): \V \subseteq \X, \bv \in supp(\V),
  (W, Z) \in \sE, Z \notin \V\} \cup \\
  &\{(U_Z, Z_{\V=\bv}, \V \subseteq \X, \bv \in supp(\V), Z \notin \V \} .
 \end{align*}
 Para todo $W \in \sV$, abreviamos $W_{\emptyset}$ por $W$.
\end{definition}
 
Em palavras, o grafo de resultados potenciais cria 
uma cópia de $\sG$ para cada possível intervenção, $\V=\bv$.
Além disso, adiciona-se uma aresta de $U_W$ para cada cópia de $W$.
Esta construção indica que as mesmas variáveis em $U$
geram todos os resultados potenciais.
Também, para cada vértice em que 
houve uma intervenção,
$W_{\V=\bv} \in \V_{\V=\bv}$, removem-se 
todas as arestas que apontam para $W_{\V=\bv}$.
Esta remoção ocorre porque, 
quando realizamos uma intervenção em $\V$
o valor desta variável é fixado e, assim,
não é gerado por suas causas em $\sG$.

\begin{example}
 \label{ex:grafo_potencial}
 Considere que $(X,Y) \in \{0,1\}^2$ e 
 o grafo causal é $X \rightarrow Y$.
 Vimos no \cref{ex:eq_struct} que 
 o grafo causal estrutural é dado por
 $U_X \rightarrow X \rightarrow Y \leftarrow U_Y$.
 Vamos construir o grafo de resultados potenciais
 dadas intervenções em $X$.
 Neste caso, além dos vértices $U_X, U_Y, X, Y$,
 temos também $X_{X=0}, Y_{X=0}, X_{X=1}, Y_{X=1}$.
 Como não há ambiguidade neste caso, 
 podemos abreviar os últimos quatro vértices por
 $X_0, Y_0, X_1, Y_1$.
 
 O grafo de resultados potenciais é
 ilustrado na \cref{fig:grafo_potencial}.
 O grafo causal estrutural é a reta horizontal 
 de $U_X$ a $U_Y$. 
 Os resultados potenciais são 
 cópias deste grafo que usam 
 as mesmas variáveis $U$ e 
 em que removemos as arestas que
 apontam para a intervenções, $X_0$ e $X_1$.
 
<<grafo_potencial, fig.pos="t", fig.height=3, fig.width=4, fig.cap="Grafo de resultados potenciais dadas intervenções em $X \\in \\{0,1\\}$.", fig.align="center", echo = FALSE, message = FALSE, warning = FALSE>>==
library(dagitty)
library(ggdag)
library(ggplot2)

# Especificar o grafo
grafo <- dagitty("dag {
    UX -> X -> Y
    UY -> { Y Y0 Y1}
    X0 -> Y0
    X1 -> Y1
}")
coordinates(grafo) <- list( 
  x=c(UX=0, X=1, Y=2, UY=3, X0 = 1, Y0 = 2, X1 = 1, Y1 = 2),
  y=c(UX=1, X=1, Y=1, UY=1, X0 = 2, Y0 = 2, X1 = 0, Y1 = 0))

# Exibir a figura do grafo
ggdag(grafo, layout = "circle") +
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank()) +
  xlab("") + ylab("")
@ 
\end{example}

Uma vez definido o grafo de resultados potenciais,
podemos extender a distribuição
do modelo de equações estruturais para este grafo.
Esta extensão envolve três etapas.
Primeiramente, a distribuição de $U$ continua a mesma.
Em segundo lugar, para todo 
vértice do grafo de resultados potenciais, $W_{\V=\bv}$, 
em que não houve uma intervenção, 
este vértice é gerado pelo mesmo mecanismo que $W$.
Isto é, $W_{\V=\bv} = \I(g_W(U_W, Pa^*(W_{\V=\bv})))$.
Finalmente, se houve uma intervenção em $W_{\V=\bv}$,
então ela é uma variável degenerada no valor desta intervenção.
Esta construção é formalizada na \cref{def:pom}.

\begin{definition}
 \label{def:pom}
 Seja $(\sG^+, f^+)$ um
 SCM para $(\sG, f)$
 com funções determinísticas, $g$.
 O modelo de resultados potenciais (POM)\footnote{
  utilizamos a sigla POM em referência ao
  termo em inglês ``potential outcomes model''} 
  dado por intervenções em $\X$,
 é um modelo probabilístico em um DAG, $(\sG^*, f^*)$, tal que
 $\sG^*$ é o grafo de resultados potenciais
 dado por intervenções em $\X$ (\cref{def:grafo_potencial}) e
 \begin{align*}
  f^*(U_W) &= f(U_W) & \text{, para todo } W \in \sV, \\
  f^*(W_{\V=\bv}|Pa^*(W_{\V=\bv}))
  &= 
  \begin{cases}
   \I(W_{\V=\bv} = \bv_i) & \text{, se } W \equiv \V_i \\
   \I(W_{\V=\bv} = g_W(U_W, Pa^*(W_{\V=\bv}))) & \text{, caso contrário.}
  \end{cases}
 \end{align*}
\end{definition}

O \cref{ex:pom} ilustra 
um modelo de resultados potenciais.

\begin{example}
 \label{ex:pom}
 Considere o SCM em equações estruturais em \cref{ex:eq_struct}.
 Na construção do modelo de resultados potenciais,
 definimos $X, Y, U_X, U_Y$ igualmente ao \cref{ex:eq_struct}.
 Além disso, para cada $x > 0$,
 $X_{x} \equiv x$ e $Y_x \equiv -log(U_Y)/X_x$.
\end{example}

\section{Levando a intuição do SCM ao POM}
\label{sec:trad_scm_pom}

Ainda que seja uma formalização conveniente,
o POM é consideravelmente mais complexo que o SCM original.
Para ganhar intuição sobre o POM,
alguns lemas de tradução são fundamentais.

\begin{lemma} 
 \label{lemma:obs_to_potential}
 Se $\V, \Z \subseteq \sV$ e
 $\V \cap \Z = \emptyset$, então
 $\P(\sV_{\V=\bv} = \sV_{\Z=\bz, \V=\bv}|\Z_{\V=\bv}=\bz) = 1$.
 Em particular,
 \begin{align*}
  \P(\sV = \sV_{\Z=\bz}|\Z = \bz) &= 1.
 \end{align*}
\end{lemma}

O \cref{lemma:obs_to_potential} conecta
o dado observacional em $\sV$ ao
resultado potencial $\sV_{\Z=\bz}$.
Mais especificamente, quando 
observamos que $\Z=\bz$, então
os resultados potenciais dada a
intervenção $\Z=\bz$ são
idênticos aos resultados observados.
Em outras palavras,
ao observarmos que $\Z=\bz$, aprendemos que
estamos justamente na hipótese
de resultados potenciais em que $\Z=\bz$.

\begin{lemma}
 \label{lemma:obs_to_potential_prev}
 Se $\V, \Z \subseteq \sV$ e
 $\V \cap \Z = \emptyset$, então
 para todo $W \in \sV$,
 \begin{align*}
  W_{\V=\bv} \I(\Z_{\V=\bv} = \bz)
  &\equiv W_{\Z = \bz, \V=\bv} \I(\Z_{\V=\bv} = \bz)
 \end{align*}
\end{lemma}

O \cref{lemma:obs_to_potential_prev} é extremamente útil,
ainda que de natureza mais técnica.
Ele permite que relacionemos resultados potenciais em que
diferentes tipos de intervenção são adotados.

Um outro resultado essencial é
o de que $\sV_{\V=\bv}$ tem
a distribuição de quando
realizamos a intervenção $do(\V=\bv)$.
Esta resultado é estabelecido
no \cref{lemma:po_do}.

\begin{lemma}
 \label{lemma:po_do}
 No modelo de resultados potenciais (\cref{def:pom}):
 \begin{align*}
  f^*(\sV_{\V=\bv})
  &\equiv f(\sV|do(\V=\bv)).
 \end{align*}
\end{lemma}

O \cref{lemma:po_do} fornece 
uma outra forma de pensar sobre o efeito causal.
Decorre do \cref{lemma:po_do} que
$\E[Y_{X=x}] = \E[Y|do(X=x)]$. Assim,
se por exemplo $X$ é binário,
$\ACE = \E[Y_1] - \E[Y_0]$.
Em outras palavras, 
como a \cref{def:pom} cria 
variáveis aleatórias que tem a distribuição intervencional,
ela permite que imaginemos o efeito causal
em termos destas variáveis.
Como no \cref{cap:intervencao} 
não havia acesso aos resultados potenciais, 
era necessário imaginar o efeito causal
somente em termos da distribuição intervencional.
Assim, a \cref{def:pom} oferece
mais formas de pensar sobre o efeito causal.\footnote{
 Esta outra forma de pensar sobre o efeito causal é
 tão relevante que outras construções de Inferência Causal,
 como o Rubin Causal Model \citep{Holland1986} partem
 diretamente dela.
}

Uma forma alternativa de pensar sobre
identificação causal está
na definição de \textit{ignorabilidade}.
Dizemos que $X$ é ignorável para
medir o efeito causal em $Y$ se
ele é independente dos resultados potenciais $Y_{x}$\footnote{
Quando não houver ambiguidade, denotaremos $Y_{X=x}$ por $Y_x$}.
Em outras palavras, 
saber o valor de $X$ não traz informação sobre
o resultado de $Y$ em uma outra realidade em que
realizamos uma intervenção sobre $X$.

\begin{definition}[Ignorabilidade]
 \label{def:ignore}
 Dizemos que $X$ é \textit{ignorável} para
 medir o efeito causal em $Y$ se
 $Y_x \dsep X$.
\end{definition}

O critério da ignorabilidade é equivalente a
afirmar que $X$ e $Y$ não tem um ancestral comum, 
isto é, um confundidor.
Em outras palavras, $X$ é ignorável se
e somente se $\emptyset$ satisfaz o critério backdoor para
medir o efeito causal de $X$ em $Y$.

\begin{lemma}
 \label{lemma:ignore_backdoor}
 As seguintes afirmações são equivalentes:
 \begin{enumerate}
  \item $\emptyset$ satisfaz o critério backdoor para
  medir o efeito causal de $X$ em $Y$,
  \item $Anc(X) \cap Anc(Y) = \emptyset$, isto é,
  $X$ e $Y$ não tem um ancestral em comum, e
  \item $X$ é ignorável para medir o efeito causal em $Y$.
 \end{enumerate}
\end{lemma}

Assim, decorre do fato de que $X$ é ignorável para 
o efeito causal em $Y$ que
a distribuição intervencional de $Y$ dado $X$ é
equivalente à sua distribuição observacional.
Em outras palavras, dizer que $X$ é ignorável tem
consequências similares a dizer que
$X$ é atribuído por aleatorização.

\begin{corollary}
 \label{cor:ignore}
 Se $X$ é ignorável para medir o efeito causal em $Y$, então
 \begin{align*}
  f(y|do(x)) &= f(y|x).
 \end{align*}
\end{corollary}

A \textit{ignorabilidade condicional} oferece
uma generalização da \cref{def:ignore}.
Dizemos que, dado $\Z$, 
$X$ é ignorável para medir o efeito causal em $Y$ se
$X$ é independente de todo $Y_x$ dado $\Z$.

\begin{definition}[Ignorabilidade condicional]
 \label{def:cignore}
 Dizemos que $X$ é \textit{condicionalmente ignorável} para
 medir o efeito causal em $Y$ dado $\Z$ se
 $Y_x \dsep X | \Z$.
\end{definition}

Se $\Z$ não tem descendentes de $X$, 
a ignorabilidade condicional é
uma restrição mais forte que o critério backdoor,
conforme formalizado no \cref{lemma:cignore_backdoor}.

\begin{lemma}
 \label{lemma:cignore_backdoor}
 Suponha que $X \notin Anc(\Z)$.
 Se $X$ é condicionalmente ignorável para 
 medir o efeito causal em $Y$ dado $\Z$, então
 $\Z$ satisfaz o critério backdoor para
 medir o efeito causal de $X$ em $Y$.
\end{lemma}

Apesar do critério backdoor e
da ignorabilidade condicional não serem equivalentes,
ambos controlam confundidores, isto é,
induzem o mesmo tipo de identificação causal:

\begin{lemma}
 \label{lemma:cignore_do}
 Se $X$ é condicionalmente ignorável para 
 medir o efeito causal em $Y$ dado $\Z$, então
 $\Z$ controla confundidores para
 medir o efeito causal de $X$ em $Y$ (\cref{def:conf_control}).
\end{lemma}

Decorre do \cref{lemma:cignore_do} que
todas as estratégia de estimação do efeito causal
estudadas na \cref{sec:backdoor} também
podem ser usadas sob 
a suposição de ignorabilidade condicional.
Em outras palavras, 
ignorabilidade condicional fornece 
um critério alternativo para
justificar o tipo de identificação causal
obtida pelo critério backdoor. 

\subsection{Exercícios}

\begin{exercise}
 Prove o \cref{lemma:simulacao}.
\end{exercise}

\begin{exercise}
 Mostre que no \cref{ex:eq_struct} 
 a distribuição de $(X,Y)$ no SCM 
 em equações estruturais é igual 
 àquela no SCM original.
\end{exercise}

\begin{exercise}
 Exiba um exemplo em que
 $X$ é condicionalmente ignorável para $Y$ dado $\Z$ mas
 $\Z$ não satisfaz o critério backdoor para
 medir o efeito causal de $X$ em $Y$.
\end{exercise}

\begin{exercise}
 Exiba um exemplo em que
 $\Z$ satisfaz o critério backdoor para
 medir o efeito causal de $X$ em $Y$ mas
 $X$ não é condicionalmente ignorável para $Y$ dado $\Z$.
\end{exercise}
