\section{Controlando confundidores (critério \textit{backdoor})}
\label{sec:backdoor}

Um confundidor é uma causa comum, 
direta ou indireta, de $X$ em $Y$.
Na existência de confundidores,
a regressão de $Y$ em $X$ no modelo observacional, 
$\E[Y|X]$, é diferente desta regressão
no modelo de intervenção, $\E[Y|do(X)]$.
Isto ocorre pois, quando calculamos $\E[Y|X]$,
utilizamos toda a informação em $X$ para prever $Y$.
Esta informação inclui não apenas 
o efeito causal de $X$ em $Y$, como 
também a informação que $X$ traz indiretamente sobre $Y$ 
pelo fato de ambas estarem associados aos seus confundidores.

Para ilustrar este raciocínio, 
podemos revisitar o \cref{ex:simpson_fim}.
uma vez que Sexo (Z) é causa comum do Tratamento (X) e
da Cura (Y), Z é um confundidor.
Quando calculamos $f(y|x)$ (\cref{eq:prob_causa_1}), 
utilizamos não só o efeito direto de $X$ em $Y$, 
expresso em $f(y|x,z)$, como também 
a informação que indireta que $X$ traz sobre $Y$
por meio do confundidor $Z$,
expressa pela combinação de $f(z|x)$ com $f(y|x,z)$.

Esta seção desenvolve uma estratégia para
medir o efeito causal chamada de 
critério \textit{backdoor}, que
consiste em bloquear 
todos os caminhos de informação que passam por confundidores:

\begin{definition}
 \label{def:backdoor}
 Seja $\sG = (\sV, \sE)$ um grafo causal e $X, Y \in \sV$.
 Dizemos que $\Z \subseteq \sV - \{X,Y\}$ satisfaz 
 o critério ``backdoor'' se:
 \begin{itemize}
  \item $X \notin Anc(\Z)$,
  \item Para todo caminho de $X$ em $Y$, 
  $C = (X, C_2, \ldots, C_{n-1}, Y)$ tal que
  $(C_2, X) \in \sE$, $C$ está bloqueado dado $\Z$.
 \end{itemize}
\end{definition}

\begin{example}
 No \cref{ex:simpson_fim} o único caminho de $X$ em $Y$ em que
 o vértice ligado a $X$ é pai de $X$ é $X \leftarrow Z \rightarrow Y$.
 Como $Z$ é um confudidor neste caminho, ele o bloqueia.
 Assim, $Z$ satisfaz o critério backdoor.
\end{example}

\begin{example}
 <<backdoor_ex_1, fig.pos="t", fig.height=2.5, fig.width=4, fig.cap="Para medir o efeito causal de X em Y, podemos aplicar o critério backdoor. Neste grafo o único caminho aplicável ao critério backdoor é (X, Z, W, Y). Neste caminho, $Z$ é uma cadeia e $W$ é um confundidor. Assim, todas as possibilidades dentre Z, W, (Z,W) bloqueiam o caminho e satisfazem o critério backdoor.", fig.align="center", echo = FALSE, message = FALSE, warning = FALSE>>==
library(dagitty)
library(ggdag)
library(ggplot2)

# Especificar o grafo
grafo <- dagitty("dag {
    X[e] Y[o]
    W -> { Z Y };
    Z -> X;
    X -> { A Y };
    A -> Y
}")
coordinates(grafo) <- list( 
  x=c(W=2, Z=0, X=0, Y=2, A= 1),
  y=c(W=1, Z=1, X=0, Y=0, A= -1))

# Exibir a figura do grafo
ggdag(grafo, layout = "circle") +
  theme(axis.text.x=element_blank(),
      axis.ticks.x=element_blank(),
      axis.text.y=element_blank(),
      axis.ticks.y=element_blank()) +
  xlab("") + ylab("")
@

 Considere o grafo causal na \cref{fig:backdoor_ex_1}.
 Para aplicar o critério backdoor, devemos identificar
 todos os caminhos de $X$ em $Y$ em que
 o vértice ligado a $X$ é pai de $X$, isto é,
 temos $X \leftarrow$. O único caminho deste tipo é:
 $X \leftarrow Z \leftarrow W \rightarrow Y$.
 Neste caminho, $Z$ é uma cadeia e $W$ é um confudidor.
 Assim, é possível bloquear este caminho condicionando
 em $Z$, em $W$, e em $(Z,W)$.
 Isto é, todos estas combinações 
 satisfazem o critério backdoor.
\end{example}

\begin{example}
 <<backdoor_ex_2, fig.pos="t", fig.height=3, fig.width=4, fig.cap="Para medir o efeito causal de X em Y, podemos aplicar o critério backdoor. Neste grafo existem dois caminhos aplicáveis ao critério backdoor: (X, A, B, Y) e (X, C, Y). No primeiro, A é um confundidor. No segundo caminho, C é um confudidor. Assim, (A,C) bloqueia ambos os caminhos e satisfaz o critério backdoor.", fig.align="center", echo = FALSE, message = FALSE, warning = FALSE>>==
library(dagitty)
library(ggdag)
library(ggplot2)

# Especificar o grafo
grafo <- dagitty("dag {
    X[e] Y[o]
    A -> { X B };
    B -> { Y }
    C -> { X Y };
    X -> { D Y };
    D -> Y
}")
coordinates(grafo) <- list( 
  x=c(A=0, B=1, C=2, D=1, X=0, Y=2),
  y=c(A=3, B=2, C=2, D=0, X=1, Y=1))

# Exibir a figura do grafo
ggdag(grafo, layout = "circle") +
  theme(axis.text.x=element_blank(),
      axis.ticks.x=element_blank(),
      axis.text.y=element_blank(),
      axis.ticks.y=element_blank()) +
  xlab("") + ylab("")
@

 Considere o grafo causal na \cref{fig:backdoor_ex_2}.
 Para aplicar o critério backdoor,
 encontramos todos os caminhos de $X$ em $Y$ em que
 o vértice ligado a $X$ é pai de $X$. 
 Há dois caminhos deste tipo:
 $X \leftarrow A \rightarrow B \rightarrow Y$ e
 $X \leftarrow C \rightarrow Y$.
 Como $A$ e $C$ são confudidores, respectivamente,
 no primeiro e segundo caminhos, 
 $(A,C)$ bloqueia ambos eles.
 Assim $(A,C)$ satisfaz o critério backdoor.
 Você consegue encontrar outro conjunto de variáveis que
 satisfaz o critério backdoor?
 
 Também é possível identificar 
 os conjuntos de variáveis que satisfazem
 o critério backdoor por meio
 do pacote \textit{dagitty},
 como ilustrado a seguir:
 
<<backdoor_ex_2_2, message = FALSE, warning = FALSE>>==
library(dagitty)
# Especificar o grafo
grafo <- dagitty("dag{
    X[e] Y[o]
    A -> { X B }; B -> { Y }; C -> { X Y };
    X -> { D Y }; D -> Y }")

adjustmentSets(grafo, type = "all")
@
\end{example}

O critério backdoor generaliza duas condições especiais que
são muito utilizadas.
Em uma primeira condição, 
o valor de $X$ é gerado integralmente por
um aleatorizador, independente de todas as demais variáveis.
Esta ideia é captada pela 
\cref{def:aleatorizacao}, abaixo:

\begin{definition}
 \label{def:aleatorizacao}
 Dizemos que $X$ é um experimento aleatorizado simples 
 se $X$ é ancestral.
\end{definition}

Em um experimento aleatorizado simples não há confundidores.
Assim, $\emptyset$ satisfaz o critério backdoor:

\begin{lemma}
 \label{lemma:backdoor_random}
 Se $X$ é um experimento aleatorizado simples, então
 $\emptyset$ satisfaz o critério backdoor.
\end{lemma}

Veremos que em um experimento aleatorizado simples a
distribuição intervencional é igual 
à distribuição observacional.
Assim, $\E[Y|do(X)] = \E[Y|X]$ e
a inferência causal é reduzida 
à inferência comumente usadas para
a distribuição observacional.

Além disso, o conjunto de todos os pais de $X$
também satisfaz o critério backdoor:

\begin{lemma}
 \label{lemma:backdoor_pais}
 $\Z = Pa(X)$ satisfaz o critério backdoor para
 medir o efeito causal de $X$ em $Y$.
\end{lemma}

A seguir, veremos como o critério backdoor permite
a identificação causal, isto é,
uma equivalência entre quantidades de interesse
obtidas pelo modelo de intervenção e
quantidades obtidas pelo modelo observacional.

\subsection{Identificação causal usando o critério backdoor}

A seguir, o \cref{thm:backdoor} mostra que,
se $\Z$ satisfaz o critério backdoor, então
$f(y|do(x))$ pode ser obtido diretamente a partir de
$f(y|x,\z)$ e $f(\z)$:

\begin{theorem}
 \label{thm:backdoor}
 Se $\Z$ satisfaz 
 o critério backdoor para medir
 o efeito causal de $X$ em $Y$, então
 \begin{align*}
  f(y|do(x))
  &= \int f(y|x,\z)f(\z)d\z
 \end{align*}
\end{theorem}

Para compreender intuitivamente o \cref{thm:backdoor},
podemos retornar ao \cref{ex:simpson_fim}.
Considere o caso em que $X, Y, Z$ são as indicadoras de que,
respectivamente, o paciente foi submetido ao tratamento,
se curou e, é de sexo masculino.
Similarmente ao \cref{thm:backdoor},
vimos em \cref{ex:simpson_fim} que $f(y|do(x))$ é
a média de $f(y|x,z)$ ponderada por $f(z)$.
Nesta ponderação, utilizamos $f(z)$ 
ao invés de $f(z|x)$ pois $Z$ é um confundidor e,
assim, no modelo intervencional não propagamos
a informação em $X$ por esta variável.
A mesma lógica se aplica 
às variáveis que satisfazem o critério backdoor.

Para calcular quantidades 
como o $\ACE$ (\cref{def:ace}), 
utilizamos $\E[Y|do(X)]$.
Por meio do \cref{thm:backdoor},
é possível obter equivalências entre
$\E[Y|do(X)]$ e esperanças obtidas
no modelo observacional.
Estas equivalências são descritas
nos \cref{thm:backdoor_ajuste,thm:backdoor_ipw}.

\begin{theorem}
 \label{thm:backdoor_ajuste}
 Se $\Z$ satisfaz 
 o critério backdoor para medir
 o efeito causal de $X$ em $Y$, então
 \begin{align*}
  \E[g(Y)|do(X=x)] 
  &= \E[\E[g(Y)|X=x,\Z]]
 \end{align*}
\end{theorem}

\begin{theorem}
 \label{thm:backdoor_ipw}
 Se $\Z$ satisfaz 
 o critério backdoor para medir
 o efeito causal de $X$ em $Y$ e
 $X$ é discreto, então
 \begin{align*}
  \E[g(Y)|do(X=x)] 
  &= \E\left[\frac{g(Y)\I(X=x)}{f(x|\Z)}\right]
 \end{align*}
\end{theorem}

A seguir, veremos como os
\cref{thm:backdoor_ajuste,thm:backdoor_ipw} podem
ser usados para estimar o efeito causal.
Para provar resultados sobre os estimadores obtidos,
a seguinte definição será útil

\begin{definition}
 \label{def:perm}
 Seja $\hat{g}$ um estimador treinado com
 os dados $(\sV_1,\ldots,\sV_n)$.
 Dizemos que $\hat{g}$ é 
 invariante a permutações se
 o estimador não depende da ordem dos dados.
 Isto é, para qualquer permutações dos índices,
 $\pi: \{1,\ldots,n\} \rightarrow \{1,\ldots,n\}$,
 $\hat{g}(\sV_1,\ldots,\sV_n) \equiv 
 \hat{g}(\sV_{\pi(1)},\ldots,\sV_{\pi(n)})$
\end{definition}

\begin{example}
 \label{ex:perm}
 A média amostral é invariante a permutações pois,
 para qualquer permutação $\pi$,
 \begin{align*}
  \frac{\sum_{i=1}^n X_i}{n} 
  = \frac{\sum_{i=1}^n X_{\pi(i)}}{n}.
 \end{align*} 
\end{example}

\subsection{Estimação usando o critério backdoor}
\label{sec:backdoor_est}

\subsubsection{Fórmula do ajuste}

O \cref{thm:backdoor_ajuste} determina que, 
se $\Z$ satisfaz o critério backdoor, então
$\E[Y|do(X=x) = \E[\E[Y|X=x,\Z]]$.
Para criar um estimador baseado nesta expressão,
analisaremos o segundo termo com mais detalhe.

Primeiramente, note que 
$\mu(X,Z) := \E[Y|X,\Z]$ é
a função de regressão de $Y$ 
em $X$ e $Z$. Assim, inicialmente
podemos utilizar quaisquer métodos
para estimação de regressão.
Por exemplo, se $Y$ é contínua,
possíveis métodos são:
regressão linear, Nadaraya-Watson, 
floresta aleatória de regressão, redes neurais, \ldots
Por outro lado, se $Y$ é discreta,
então a função de regressão é estimada por
métodos de classificação como:
regressão logística, k-NN, 
floresta aleatória de classificação,
redes neurais, \ldots
Para qualquer opção escolhida,
denotamos o estimador de $\mu$ por $\hmu$.
Tendo ajustado um estimador, 
podemos argumentar que:
\begin{align*}
 \E[\E[Y|X=x,\Z]] &= \E[\mu(x,\Z)] \approx \E[\hmu(x,\Z)]
\end{align*}
Finalmente, $\E[\hmu(x,\Z)]$ é simplesmente uma média,
que podemos aproximar usando a Lei dos Grandes Números:
\begin{align*}
 \E[\hmu(x,\Z)] &\approx \frac{\sum_{i=1}^n \hmu(x,\Z_i)}{n}
\end{align*}
Combinando estas conclusões, 
obtemos o estimador pela fórmula do ajuste, $\hdoy$:

\begin{definition}
 \label{def:ajuste}
 Considere que $\Z$ satisfaz o critério backdoor para
 medir o efeito causal de $X$ em $Y$ e
 $\hmu(x,\z)$ é uma estimativa 
 da regressão $\E[Y|X=x,\Z=\z]$.
 O estimador de $\E[Y|do(X=x)]$ pela
 fórmula do ajuste é:
\begin{align*}
 \hdoy &:= \frac{\sum_{i=1}^n \hmu(x,\Z_i)}{n}
\end{align*}
\end{definition}

A seguir mostraremos que,
se $\hmu$ converge para $\mu$, então
$\hdoy$ converge para $\E[Y|do(X=x)]$.
Em outras palavras, é possível utilizar
$\hdoy$ para estimar o efeito causal de $X$ em $Y$
por meio de expressões como o $\ACE$.

\begin{theorem}
 \label{thm:conv_ajuste}
 Seja $\mu(X,\Z) := \E[Y|X,\Z]$.
 Se $\Z$ satisfaz o critério backdoor para
 medir o efeito causal de $X$ em $Y$,
 $\E[|\mu(x,\Z_1)|] < \infty$,
 $\E[|\hmu(x,\Z_1)-\mu(x,\Z_1)|] = o(1)$, 
 e $\hmu$ é invariante a permutações (\cref{def:perm}), então
 $\hdoy \convp \E[Y|do(X=x)]$.
\end{theorem}

A seguir, utilizamos dados simulados para
ilustrar a implementação da fórmula do ajuste.

\begin{example}
 \label{ex:backdoor_est_ajuste}
<<backdoor_est_ex, fig.pos="t", fig.height=2.5, fig.width=4, fig.cap="DAG usado como exemplo para estimar efeito de X em Y.", fig.align="center", echo = FALSE, message = FALSE, warning = FALSE>>==
# Especificar o grafo
grafo <- dagitty("dag {
    X[e] Y[o]
    {A B} -> { X Y }; X -> {C Y}; C -> Y
}")
coordinates(grafo) <- list( 
  x=c(A=0, B=2, X=0, Y=2, C= 1),
  y=c(A=1, B=1, X=0, Y=0, C= -1))

# Exibir a figura do grafo
ggdag(grafo, layout = "circle") +
  theme(axis.text.x=element_blank(),
      axis.ticks.x=element_blank(),
      axis.text.y=element_blank(),
      axis.ticks.y=element_blank()) +
  xlab("") + ylab("")
@

Considere que o grafo causal é
dado pela \cref{fig:backdoor_est_ex}.
Vamos supor que os dados são gerados da seguinte forma:
$\sigma^2 = 0.01$,
$A \sim N(0, \sigma^2)$, 
$B \sim N(0, \sigma^2)$,
$\epsilon \sim Bernoulli(0.95)$
$X \equiv \I(A + B > 0)\epsilon + \I(A + B < 0)(1-\epsilon)$,
$C \sim N(X, \sigma^2)$, e
$Y \sim N(A + B + C + X, \sigma^2)$:

<<backdoor_est_ex_2, message = FALSE, warning = FALSE>>==
# Especificar o grafo
grafo <- dagitty::dagitty("dag {
    X[e] Y[o]
    {A B} -> { X Y }; X -> {C Y}; C -> Y }")

# Simular os dados
n <- 10^5
sd = 0.1
A <- rnorm(n, 0, sd)
B <- rnorm(n, 0, sd)
eps <- rbinom(n, 1, 0.8)
X <- as.numeric(eps*((A + B) > 0) + 
                  (1-eps)*((A + B) <= 0))
C <- rnorm(n, X, sd)
Y <- rnorm(n, A + B + C + X, sd)
data <- dplyr::tibble(A, B, C, X, Y)
@

A seguir, estimaremos o efeito causal pela
fórmula do ajuste (\cref{def:ajuste}).
Iniciaremos a análise utilizando $\hmu$ como
sendo uma regressão linear simples:

<<backdoor_est_ex_3>>==
# Sejam Z variáveis que satisfazem o critério backdoor para
# estimar o efeito causal de causa em efeito em grafo.
# Retorna uma fórmula do tipo Y ~ X + Z_1 + ... + Z_d
fm_ajuste <- function(grafo, causa, efeito)
{
  var_backdoor <- dagitty::adjustmentSets(grafo)[[1]]
  regressores = c(causa, var_backdoor)
  fm = paste(regressores, collapse = "+")
  fm = paste(c(efeito, fm), collapse = "~")
  as.formula(fm)
}

# Estima E[Efeito|do(causa = x)] pela 
# formula do ajuste usando mu_chapeu como regressao 
est_do_x_lm <- function(data, mu_chapeu, causa, x)
{
  data %>% 
    dplyr::mutate({{causa}} := x) %>%  
    predict(mu_chapeu, newdata = .) %>% 
    mean()
}

# Estimação do ACE com regressão linear simples
fm <- fm_ajuste(grafo, "X", "Y")
mu_chapeu_lm <- lm(fm, data = data)
ace_ajuste_lm = est_do_x_lm(data, mu_chapeu_lm, "X", 1) - 
  est_do_x_lm(data, mu_chapeu_lm, "X", 0)
round(ace_ajuste_lm)
@

Em alguns casos, não é razoável supor que 
$\E[Y|X,\Z]$ é linear. Nestas situações,
é fácil adaptar o código anterior para
algum método não-paramétrico arbitrário.
Exibimos uma implementação usando XGBoost 
\citep{Chen2023}:

<<backdoor_est_ex_4, message = FALSE, warning = FALSE>>==
library(xgboost)
var_backdoor <- dagitty::adjustmentSets(grafo, "X", "Y")[[1]]
mu_chapeu <- xgboost(
  data = data %>% 
    dplyr::select(all_of(c(var_backdoor, "X"))) %>% 
    as.matrix(),
  label = data %>% 
    dplyr::select(Y) %>% 
    as.matrix(),
  nrounds = 100,
  objective = "reg:squarederror",
  early_stopping_rounds = 3,
  max_depth = 2,
  eta = .25,
  verbose = FALSE
)

est_do_x_xgb <- function(data, mu_chapeu, causa, x)
{
  data %>% 
    dplyr::mutate({{causa}} := x) %>%  
    dplyr::select(c(var_backdoor, causa)) %>% 
    as.matrix() %>% 
    predict(mu_chapeu, newdata = .) %>% 
    mean()
}

ace_est_xgb = est_do_x_xgb(data, mu_chapeu, "X", 1) - 
  est_do_x_xgb(data, mu_chapeu, "X", 0)
round(ace_est_xgb, 2)
@

 Como o modelo linear era adequado para $\E[Y|X,\Z]$,
 não vemos diferença entre a estimativa obtida
 pela regressão linear simples e pelo XGBoost.
 Mas será que as estimativas estão adequadas?
 Como simulamos os dados,
 é possível calcular diretamente $\E[Y|do(X=x)]$:
 \begin{align}
  \label{eq:ex_backdoor_1}
  \E[Y|do(X=x)]
  &= \E[\E[Y|X=x, A, B]]
  & \text{\cref{thm:backdoor_ajuste}} \nonumber \\
  &= \E[\E[\E[Y|X=x, A, B, C]|X=x, A, B]]
  & \text{Lei da esperança total} \nonumber \\
  &= \E[\E[A+B+C+X|X=x, A, B]]
  & Y \sim N(A+B+C+X, \sigma^2) \nonumber \\
  &= \E[A + B + 2x]
  & C \sim N(X, \sigma^2) \nonumber \\
  &= 2x
  & \E[A] = \E[B] = 0
 \end{align}
 Uma vez calculado $\E[Y|do(X=x)]$,
 podemos obter o $\ACE$:
 \begin{align*}
  \ACE
  &= \E[Y|do(X=1)] - \E[Y|do(X=0)] \\
  &= 2 \cdot 1 - 2 \cdot 0 = 2
  & \text{\cref{eq:ex_backdoor_1}}
 \end{align*}
 Portanto, as estimativas do $\ACE$ obtidas 
 pela regressão linear e pelo xgboost 
 estão adequadas.
\end{example}

\subsubsection{Ponderação pelo inverso do escore de propensidade (IPW)}

Uma outra forma de estimar $\E[Y|do(X=x)]$ é
motivada pelo \cref{thm:backdoor_ipw}.
Este resultado determina que, 
se $\Z$ satisfaz o critério backdoor, então
\begin{align*}
 \E[Y|do(X=x)] 
 &= \E\left[\frac{Y \I(X=x)}{f(x|\Z)}\right].
\end{align*}
Na segunda expressão, $f(x|\z)$ é 
usualmente chamado de \textit{escore de propensidade}.
Este escore captura a forma como 
os confundidores atuam sobre $X$
nos dados observacionais.
Como $f$ em geral é desconhecido,
$f(x|\z)$ também o é.
Contudo, quando $X$ é discreto,
podemos estimar $f(x|\z)$ utilizando
algum algoritmo arbitrário de classificação.
Denotaremos esta estimativa por $\hf(x|\z)$.
Se a estimativa for boa, temos
\begin{align*}
 \E\left[\frac{Y \I(X=x)}{f(x|\Z)}\right]
 &\approx \E\left[\frac{Y \I(X=x)}{\hf(x|\Z)}\right].
\end{align*}
Finalmente, observe novamente que 
podemos aproximar a esperança por
uma média empírica, isto é,
\begin{align*}
 \E\left[\frac{Y \I(X=x)}{\hf(x|Z)}\right]
 &\approx n^{-1} \sum_{i=1}^n \frac{Y_i \I(X_i=x)}{\hf(x|\Z_i)}.
\end{align*}
Combinando estas aproximações, obtemos:
\begin{definition}
 \label{def:ipw}
 Considere que $\Z$ satisfaz o critério backdoor para
 medir o efeito causal de $X$ em $Y$ e
 $\hf(x|\z)$ é uma estimativa de $f(x|\z)$.
 O estimador de $\E[Y|do(X=x)]$ por IPW é:
\begin{align*}
 \hdoyb &:= n^{-1} \sum_{i=1}^n \frac{Y_i \I(X_i=x)}{\hf(x|\Z_i)}.
\end{align*}
\end{definition}

\begin{theorem}
 \label{thm:conv_ipw}
 Se $\hf$ é invariante a permutações (\cref{def:perm}),
 $\E[|\hf(x|\Z_1)-f(x|\Z_1)|] = o(1)$, e
 existe $M > 0$ tal que
 $\sup_\z \E[|Y| \I(X=x)|\Z=\z] < M$, e
 existe $\delta > 0$ tal que
 $\inf_z \min\{f(x|\Z_1), \hf(x|\Z_1)\} > \delta$, então
 $\hdoyb \convp \E[Y|do(X=x)]$.
\end{theorem}

A seguir, utilizamos novamente dados simulados para
ilustrar a implementação de IPW:

\begin{example}
\label{ex:backdoor_est_ipw}
Considere que o grafo causal e
o modelo de geração dos dados são 
idênticos àqueles do \cref{ex:backdoor_est_ajuste}.
Iniciaremos a análise utilizando 
regressão logística para estimar $\hf$.
 
 <<backdoor_est_ex_5>>==
# Sejam Z variáveis que satisfazem o critério backdoor para
# estimar o efeito causal de causa em efeito em grafo.
# Retorna uma fórmula do tipo X ~ Z_1 + ... + Z_d
fm_ipw <- function(grafo, causa, efeito)
{
  var_backdoor <- dagitty::adjustmentSets(grafo)[[1]]
  fm = paste(var_backdoor, collapse = "+")
  fm = paste(c(causa, fm), collapse = "~")
  as.formula(fm)
}

# Estimação do ACE por IPW onde
# Supomos X binário e
# f_1 é o vetor P(X_i=1|Z_i)
ACE_ipw <- function(data, causa, efeito, f_1)
{
  data %>% 
  mutate(f_1 = f_1,
         est_1 = {{efeito}}*({{causa}}==1)/f_1,
         est_0 = {{efeito}}*({{causa}}==0)/(1-f_1)
  ) %>% 
  summarise(do_1 = mean(est_1),
            do_0 = mean(est_0)) %>% 
  mutate(ACE = do_1 - do_0) %>% 
  dplyr::select(ACE)
}

fm <- fm_ipw(grafo, "X", "Y")
f_chapeu <- glm(fm, family = "binomial", data = data)
f_1_lm <- predict(f_chapeu, type = "response")
ace_ipw_lm <- data %>% ACE_ipw(X, Y, f_1_lm) %>% as.numeric()
ace_ipw_lm %>% round(2)
@

Também é fácil adaptar o código acima para
estimar $\ACE$ por IPW utilizando
algum método não-paramétrico para estimar $\hf$.
Abaixo há um exemplo utilizando o XGBoost:

 <<backdoor_est_ex_6>>==
var_backdoor <- dagitty::adjustmentSets(grafo)[[1]]
f_chapeu <- xgboost(
  data = data %>% 
    dplyr::select(all_of(var_backdoor)) %>% 
    as.matrix(),
  label = data %>% 
    dplyr::select(X) %>% 
    as.matrix(),
  nrounds = 100,
  objective = "binary:logistic",
  early_stopping_rounds = 3,
  max_depth = 2,
  eta = .25,
  verbose = FALSE
)

covs <- data %>% dplyr::select(all_of(var_backdoor)) %>% as.matrix()
f_1 <- predict(f_chapeu, newdata = covs)
data %>% ACE_ipw(X, Y, f_1) %>% as.numeric() %>% round(2)
@
\end{example}

\subsubsection{Estimador duplamente robusto}

Os \cref{thm:conv_ajuste,thm:conv_ipw} mostram que,
sob suposições diferentes, 
$\hdoy$ e $\hdoyb$ convergem para $\E[Y|do(X=x)]$.
A ideia do estimador duplamente robusto é
combinar ambos os estimadores de forma
a garantir esta convergência sob 
suposições mais fracas.
Para tal, a ideia por trás do 
estimador duplamente é que
este convirja junto a $\hdoy$ 
quando este é consistente e
para $\hdoyb$ quando aquele o é.

\begin{definition}
 Sejam $\Z$ variáveis que satisfazem o critério backdoor
 para medir o efeito causal de $X$ em $Y$ e 
 sejam $\hf$ e $\hmu$ tais quais nas
 \cref{def:ajuste,def:ipw}.
 O estimador duplamente robusto para
 $\E[Y|do(X=x)]$, $\hdoyc$ é tal que
 \label{def:duplo_robusto}
 \begin{align*}
  \hdoyc 
  &= \hdoy + \hdoyb 
  - \sum_{i=1}^n \frac{\I(X_i=x)\hmu(x,\Z_i)}{n\hat{f}(x|\Z_i)}
 \end{align*}
\end{definition}

O estimador duplamente robusto é consistente para
$\E[Y|do(X=x)]$ tanto sob as condições do
\cref{thm:conv_ajuste} quanto sob as do
\cref{thm:conv_ipw}. A ideia básica é que,
sob as condições do \cref{thm:conv_ajuste},
$\hdoy$ é consistente para $\E[Y|do(X=x)]$ e
$\hdoyb - \sum_{i=1}^n \frac{\I(X_i=x)\hmu(x,\Z_i)}{n\hat{f}(x|\Z_i)}$
converge para $0$. Isto é,
quando $\hdoy$ é consistente,
o estimador duplamente robusto seleciona este termo.
Similarmente, sob as condições do \cref{thm:conv_ipw},
$\hdoyb$ é consistente para $\E[Y|do(X=x)]$ e
$\hdoy - \sum_{i=1}^n \frac{\I(X_i=x)\hmu(x,\Z_i)}{n\hat{f}(x|\Z_i)}$
converge para $0$.

\begin{theorem}
 \label{thm:conv_duplo_robusto}
 Suponha que existe $\epsilon > 0$ tal que
 $\inf_\z \hf(x|\z) > \epsilon$,
 existe $M > 0$ tal que 
 $\sup_\z \hmu(x,\z) < M$, e
 $\hmu$ e $\hf$ são invariantes 
 a permutações (\cref{def:perm}).
 Se as condições do \cref{thm:conv_ajuste} ou
 do \cref{thm:conv_ipw} estão satisfeitas, então
 \begin{align*}
  \hdoyc \convp \E[Y|do(X=x)].
 \end{align*}
\end{theorem}

\begin{example}[Estimador duplamente robusto]
\label{ex:backdoor_est_robusto}
Considere que o grafo causal e
o modelo de geração dos dados são iguais
àqueles descritos no \cref{ex:backdoor_est_ajuste}.
Para implementar o estimador duplamente robusto
combinaremos o estimador da fórmula do ajuste obtido 
por regressão linear no \cref{ex:backdoor_est_ajuste} e
aquele de IPW por regressão logística
no \cref{ex:backdoor_est_ipw}.
<<backdoor_est_ex_7>>==
mu_1_lm <- data %>% 
  dplyr::mutate(X = 1) %>%
  predict(mu_chapeu_lm, newdata = .)
mu_0_lm <- data %>% 
  dplyr::mutate(X = 0) %>%
  predict(mu_chapeu_lm, newdata = .)
corr <- data %>% 
  mutate(mu_1 = mu_1_lm, 
         mu_0 = mu_0_lm,
         f_1 = f_1_lm,
         corr_1 = (X == 1)*mu_1/f_1,
         corr_0 = (X == 0)*mu_0/(1-f_1)) %>% 
  summarise(corr_1 = mean(corr_1),
            corr_0 = mean(corr_0)) %>% 
  mutate(corr = corr_1 - corr_0) %>% 
  dplyr::select(corr) %>% 
  as.numeric()
ace_rob_lm <- ace_ajuste_lm + ace_ipw_lm - corr
ace_rob_lm %>% round(2)
@ 
\end{example}

\subsection{Exercícios}

\begin{exercise}
 Prove o \cref{lemma:backdoor_random}.
\end{exercise}

\begin{exercise}
 Prove o \cref{lemma:backdoor_pais}.
\end{exercise}

\begin{exercise}
 Prove que a variância amostral satisfaz
 o \cref{def:perm}.
\end{exercise}

\begin{exercise}
 Utilizando como referência o grafo e
 o código no \cref{ex:backdoor_est_ajuste},
 simule dados tais que a estimativa do $\ACE$ é
 diferente quando um método de regressão linear e
 um de regressão não-paramétrica são usados.
\end{exercise}
